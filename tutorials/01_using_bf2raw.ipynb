{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56ee9289-4f67-4216-badd-6909331a2c9e",
   "metadata": {},
   "source": [
    "# Using bioformats2raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf144f0-304b-4b19-a3f4-7cfc30afa494",
   "metadata": {},
   "source": [
    "## The bioformats2raw executable\n",
    "\n",
    "bioformats2raw is a tool for converting images into the OME-NGFF format, which is zarr-based. The best way to use bioformats2raw is via a Singularity container. The definition file for such a container can be found in the `containers/` directory in this repo.\n",
    "\n",
    "The documentation for bioformats2raw isn't great, but the usage information is quite comprehensive.\n",
    "\n",
    "```\n",
    "Usage: <main class> [-p] [--no-hcs] [--[no-]nested] [--no-ome-meta-export]\n",
    "                    [--no-root-group] [--overwrite]\n",
    "                    [--use-existing-resolutions] [--version] [--debug\n",
    "                    [=<logLevel>]] [--extra-readers[=<extraReaders>[,\n",
    "                    <extraReaders>...]]]... [--options[=<readerOptions>[,\n",
    "                    <readerOptions>...]]]... [-s[=<seriesList>[,\n",
    "                    <seriesList>...]]]...\n",
    "                    [--additional-scale-format-string-args=<additionalScaleForma\n",
    "                    tStringArgsCsv>] [-c=<compressionType>]\n",
    "                    [--dimension-order=<dimensionOrder>]\n",
    "                    [--downsample-type=<downsampling>]\n",
    "                    [--fill-value=<fillValue>] [-h=<tileHeight>]\n",
    "                    [--max_cached_tiles=<maxCachedTiles>]\n",
    "                    [--max_workers=<maxWorkers>]\n",
    "                    [--memo-directory=<memoDirectory>]\n",
    "                    [--pixel-type=<outputPixelType>]\n",
    "                    [--pyramid-name=<pyramidName>] [-r=<pyramidResolutions>]\n",
    "                    [--scale-format-string=<scaleFormatString>]\n",
    "                    [--target-min-size=<minSize>] [-w=<tileWidth>]\n",
    "                    [-z=<chunkDepth>]\n",
    "                    [--compression-properties=<String=Object>]...\n",
    "                    [--output-options=<String=String>[\\|<String=String>...]]...\n",
    "                    <inputPath> <outputLocation>\n",
    "      <inputPath>            file to convert\n",
    "      <outputLocation>       path to the output pyramid directory. The given\n",
    "                               path can also be a URI (containing ://) which\n",
    "                               will activate **experimental** support for\n",
    "                               Filesystems. For example, if the output path\n",
    "                               given is 's3://my-bucket/some-path' *and* you\n",
    "                               have an S3FileSystem implementation in your\n",
    "                               classpath, then all files will be written to S3.\n",
    "      --additional-scale-format-string-args=<additionalScaleFormatStringArgsCsv>\n",
    "                             Additional format string argument CSV file\n",
    "                               (without header row).  Arguments will be added\n",
    "                               to the end of the scale format string mapping\n",
    "                               the at the corresponding CSV row index.  It is\n",
    "                               expected that the CSV file contain exactly the\n",
    "                               same number of rows as the input file has series\n",
    "  -c, --compression=<compressionType>\n",
    "                             Compression type for Zarr (null, zlib, blosc;\n",
    "                               default: blosc)\n",
    "      --compression-properties=<String=Object>\n",
    "                             Properties for the chosen compression (see https:\n",
    "                               //jzarr.readthedocs.io/en/latest/tutorial.\n",
    "                               html#compressors )\n",
    "      --debug, --log-level[=<logLevel>]\n",
    "                             Change logging level; valid values are OFF, ERROR,\n",
    "                               WARN, INFO, DEBUG, TRACE and ALL. (default: WARN)\n",
    "      --dimension-order=<dimensionOrder>\n",
    "                             Override the input file dimension order in the\n",
    "                               output file [Can break compatibility with\n",
    "                               raw2ometiff] (XYZCT, XYZTC, XYCTZ, XYCZT, XYTCZ,\n",
    "                               XYTZC)\n",
    "      --downsample-type=<downsampling>\n",
    "                             Tile downsampling algorithm (SIMPLE, GAUSSIAN,\n",
    "                               AREA, LINEAR, CUBIC, LANCZOS)\n",
    "      --extra-readers[=<extraReaders>[,<extraReaders>...]]\n",
    "                             Separate set of readers to include; (default:\n",
    "                               [class com.glencoesoftware.bioformats2raw.\n",
    "                               PyramidTiffReader, class com.glencoesoftware.\n",
    "                               bioformats2raw.MiraxReader, class com.\n",
    "                               glencoesoftware.bioformats2raw.BioTekReader])\n",
    "      --fill-value=<fillValue>\n",
    "                             Default value to fill in for missing tiles (0-255)\n",
    "                               (currently .mrxs only)\n",
    "  -h, --tile_height=<tileHeight>\n",
    "                             Maximum tile height to read (default: 1024)\n",
    "      --max_cached_tiles=<maxCachedTiles>\n",
    "                             Maximum number of tiles that will be cached across\n",
    "                               all workers (default: 64)\n",
    "      --max_workers=<maxWorkers>\n",
    "                             Maximum number of workers (default: 1)\n",
    "      --memo-directory=<memoDirectory>\n",
    "                             Directory used to store .bfmemo cache files\n",
    "      --no-hcs               Turn off HCS writing\n",
    "      --[no-]nested          Whether to use '/' as the chunk path seprator\n",
    "                               (true by default)\n",
    "      --no-ome-meta-export   Turn off OME metadata exporting [Will break\n",
    "                               compatibility with raw2ometiff]\n",
    "      --no-root-group        Turn off creation of root group and corresponding\n",
    "                               metadata [Will break compatibility with\n",
    "                               raw2ometiff]\n",
    "      --options[=<readerOptions>[,<readerOptions>...]]\n",
    "                             Reader-specific options, in format key=value[,\n",
    "                               key2=value2]\n",
    "      --output-options=<String=String>[\\|<String=String>...]\n",
    "                             |-separated list of key-value pairs to be used as\n",
    "                               an additional argument to Filesystem\n",
    "                               implementations if used. For example,\n",
    "                               --output-options=s3fs_path_style_access=true|...\n",
    "                               might be useful for connecting to minio.\n",
    "      --overwrite            Overwrite the output directory if it exists\n",
    "      --pixel-type=<outputPixelType>\n",
    "                             Pixel type to write if input data is  float or\n",
    "                               double (int8, int16, int32, uint8, uint16,\n",
    "                               uint32, float, double, complex, double-complex,\n",
    "                               bit)\n",
    "      --pyramid-name=<pyramidName>\n",
    "                             Name of pyramid (default: null) [Can break\n",
    "                               compatibility with raw2ometiff]\n",
    "      -r, --resolutions=<pyramidResolutions>\n",
    "                             Number of pyramid resolutions to generate\n",
    "      -s, --series[=<seriesList>[,<seriesList>...]]\n",
    "                             Comma-separated list of series indexes to convert\n",
    "      --scale-format-string=<scaleFormatString>\n",
    "                             Format string for scale paths; the first two\n",
    "                               arguments will always be series and resolution\n",
    "                               followed by any additional arguments brought in\n",
    "                               from `--additional-scale-format-string-args`\n",
    "                               [Can break compatibility with raw2ometiff]\n",
    "                               (default: %d/%d)\n",
    "      --target-min-size=<minSize>\n",
    "                             Specifies the desired size for the largest XY\n",
    "                               dimension of the smallest resolution, when\n",
    "                               calculating the number of resolutions generate.\n",
    "                               If the target size cannot be matched exactly,\n",
    "                               the largest XY dimension of the smallest\n",
    "                               resolution should be smaller than the target\n",
    "                               size.\n",
    "      --use-existing-resolutions\n",
    "                             Use existing sub resolutions from original input\n",
    "                               format[Will break compatibility with raw2ometiff]\n",
    "      -w, --tile_width=<tileWidth>\n",
    "                             Maximum tile width to read (default: 1024)\n",
    "      -z, --chunk_depth=<chunkDepth>\n",
    "                             Maximum chunk depth to read (default: 1)\n",
    "      -p, --progress         Print progress bars during conversion\n",
    "      --version              Print version information and exit\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ed445-efb0-46d6-8d34-cfeac4fef5ae",
   "metadata": {},
   "source": [
    "The most important options to worry about are:\n",
    "\n",
    "* `-h`, `-w`, `-z` - These set the size of your zarr chunks.\n",
    "* `-c, --compression` - This selects a compression type. You generally are going to want to use some kind of compression, as it can keep your file size way down.\n",
    "* `--max_workers` - This is used to parallelize the conversion, which can speed things up dramatically. This should generally be set to the number of available cores, and never higher than that.\n",
    "* `--[no-]nested` - This determines how the chunks are organized with respect to the object storage/file system. For most purposes, you'll always want to use `--no-nested` rather than `--nested`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef2e2d-69ff-4e9c-bbce-9794e6a081e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example of a bash script for submitting bioformats2raw jobs to the cluster\n",
    "\n",
    "To make using bioformats2raw more convenient on our HPC cluster, there is a script located in the `scripts/` directory in this repo that you might find useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09e242fc-b32a-41a4-bc98-a19cc919df9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf2raw.sh [-h] [-i input_file -o output_zarr -b bf2raw] -- Convert a single image file to zarr.\n",
      "\n",
      "Note that slurm submission parameters may need to be adjusted in the script for batch jobs. Note also that bioformats2raw parameters may need to be adjusted in the script depending on image shape and compression options.\n",
      "\n",
      "where:\n",
      "    -h  show this help text\n",
      "    -i  path to original image file\n",
      "    -o  path to output zarr\n",
      "    -b  path to bioformats2raw singularity container\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The \"!\" in this line is telling Jupyter to run the subsequent code as a bash command.\n",
    "# You wouldn't type the \"!\" when you are working in a terminal!\n",
    "!../scripts/bf2raw.sh -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1c400c-bdb3-4a92-9b95-6ee9847837bc",
   "metadata": {},
   "source": [
    "Here is an example of launching the script as a batch job to convert a single ndpi file (slide scan) to zarr:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a5af15-41ef-4843-98c8-4bf6f6474d9d",
   "metadata": {},
   "source": [
    "```\n",
    "sbatch ../scripts/bf2raw.sh \\\n",
    "       -i /projects/researchit/djme/zarr_dask_examples/example_data/histopath_example.ndpi \\\n",
    "       -o /projects/researchit/djme/zarr_dask_examples/zarrs/histopath_example.zarr \\\n",
    "       -b /projects/researchit/djme/containers/bf2raw_0.4.0.sif\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51874512-87f4-4d91-b354-a31864057775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
